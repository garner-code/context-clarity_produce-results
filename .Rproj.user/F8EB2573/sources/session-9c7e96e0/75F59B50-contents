---
title: "routine-produce-results"
format: 
  html:
    code-fold: TRUE
    default-image-extension: svg
editor: visual
---

## Produce the results for 'Towards the definition and measurement of routines and their impact on cognitive control'

This notebook provides a computationally reproducible record of the analysis and figure generation for the paper 'Towards a normative theory of routines' (or whatever we wind up deciding to call it.

### Settings and other things

This notebook assumes you have the following file structure. Note that the pre-existing csv files were generated by the code in [this repository.](https://github.com/garner-code/doors), and the .Rda file was generated in the previous dopamine study using code in [this repository](https://github.com/garner-code/DA_VisRoutes).

``` markdown
routines_produce-results/
│   ├── _quarto.yml
│   ├── *.Rproj
│   ├── this-qmd-doc.qmd
│   ├── R/
│   │   └── all-r-scripts.R
│   ├── data-wrangled/
│   │   └── exp_[exp_str]_evt.csv
│   │   └── exp_[exp_str]_avg.csv
│   │   └── exp_lt_maggi-k4.csv
│   │   └── dat4_seq_model.Rda ### need to find origin of this file and make consistent across locations
│   ├── figs/
│   ├── sims/
│   ├── res/
│   ├── analysis-output/
```

First, load required packages and set the relative paths for data and other required things...

```{r, fold=TRUE, results='hide'}
options(tidyverse.quiet = TRUE)
library(tidyverse)
library(grid)
library(gridExtra)
library(knitr)
library(magick)
library(ggpubr)
library(vioplot)
library(rstatix)
library(emmeans)
library(afex)
library(pdftools)
library(purrr)
library(GGally)

data_path = 'data-wrangled/' # for all data derivs
sim_data_path = 'sims/' # for simulation results
fig_path = 'figs/' # for figures
res_path = 'res/' # for inferential results

function_loc <- "R" # where are the functions?
req_functions <- list.files(function_loc)
sapply(req_functions, function(x) source(paste(here::here(function_loc),
                                               x, sep="/")))

# # now some font settings
library(extrafont)
#font_import() # run this once only, comment out after first time
loadfonts(device='pdf')
```

## Q1. Can we produce routines in a consistent way across sessions and conditions?

Here, I calculate the routine (R) scores for each participant, generate density plots of those scores over experiments and conditions, and then I print out the analysis of the basis differences between conditions, across replications.

### Compute R Scores

First I compute R scores for each participant, using the formula -

![](figs/R_def.png){width="390"}

```         
![r formula](figs/R_def.png){fig-align="cent"}
```

```{r, fold=TRUE, results='hide'}

apply_sort_data <- function(exp_str, data_path){
    # take the trial level data from session 2 and sort into contexts. We assume a person believes they are in the same context until their first 'cc' hit after a context switch.
  data_fname <- paste(data_path, 'exp_', exp_str,
                       '_evt.csv', sep='')
  save_fname <- paste(data_path, 'exp_', exp_str,
                          '_door_selections_for_ent.csv', sep='')
  ent_initial_data_sort(data_fname, save_fname)
}
lapply(c('lt', 'ts'), apply_sort_data, data_path=data_path)

apply_compute_R <- function(exp_str){
# take the data output from the step above, and compute the routine score for each participant and context
  data_fname <- paste(data_path, 'exp_', exp_str,
                      '_door_selections_for_ent.csv', sep='')
  save_new_data_fname <- paste(data_path, 'exp_', exp_str,
                               '_rscore-full.csv', sep='') # for saving R scores not averaged across context. Note that original plotting code in the function below, now commented out, shows that
  save_sum_data_fname <- paste(data_path, 'exp_', exp_str,
                               '_rscore.csv', sep='')
  ent_compute_routine_measure(data_fname, save_new_data_fname, save_sum_data_fname)
}

lapply(c('lt', 'ts'), apply_compute_R)
```

Now that we have the R scores, we can investigate whether we elicit R scores that are greater than you would expect by chance, measure something reliable in humans, and we want to get a feel for what the behaviour is that we are quantifying.

I will make 3 subplots: one showing the z-scores for humans vs agents, one showing the reliability of the measure over sessions, and one showing the trajectories for 2 participants. These will then be put together into one big figure, for the manuscript

### Trajectories

Note that the below 'function' contains hard coded, handpicked subsets of trials, to show the trajectories in performance.

```{r, results='hide', output=FALSE}

exp_str <- 'lt'
traj_data_fname <- paste(data_path, 'exp_', exp_str, '_evt.csv', sep="")
traj_fig_fname <- paste(fig_path, 'trajectories', sep='')
w <- 15 # width of plot, in cm
traj_plt <- plot_trajectories(traj_data_fname, traj_fig_fname, w)


```

Now we've created it, lets display it -

::: {#fig-trajectories layout-ncol="1"}
![](figs/trajectories.svg)

Fig 1: example trajectories across doors
:::

### R scores are systematically higher than you would expect by chance

The next thing we want to demonstrate is the R scores are statistically higher than you would expect by chance. To achieve this, we first generate a null distribution of door selections for each subject, and obtaining a z score for their observed R score and the mean and sd of the null. We will compare this to zero, and to the performance of a perfectly performing, random agent.

First, to make the nulls, we take their observed data, and resample it a thousand times, under the constraint that the same door can't be sampled twice.

Note that the evaluation of the code block below is set to false, as it takes a while to get the null distributions

```{r, eval=FALSE}

set.seed(42) # for reproducibility is the meaning of life
exp_strs = c('lt', 'ts')

apply_generate_nulls <- function(exp_str){
  door_selections_fname = paste(data_path, 'exp_', exp_str, '_door_selections_for_ent.csv', sep='')
  r_dat_fname = paste(data_path, 'exp_', exp_str, '_rscore-full.csv', sep='')
  null_rs_fname = paste(data_path, 'exp_', exp_str, '_rnulls.csv', sep='') # for saving
  generate_nulls(door_selections_fname,
                 r_dat_fname,
                 null_rs_fname)
}
lapply(exp_strs, apply_generate_nulls)

```

Now that we have the null Rs, we can compute a z-score for each participant, based on their observed R, and the null distribution. We will save the z-scores for plotting, and save the results of t-tests against zero and against -2 to a csv file. Note that when the below was originally run, visual inspection of the data showed some outliers, so I removed participants with a z-score \> or \< 3 standard deviations from the mean.

```{r, results='hide'}

exp_strs <- c('lt','ts')

z_comps <- function(exp_str, data_path){
  null_rs_fname = paste(data_path, 'exp_', exp_str, '_rnulls.csv', sep='') # for saving
  zs_out_fname = paste(data_path, 'exp_', exp_str, '_zs_cl.csv', sep='')
  zs_stats_fname = paste(res_path, 'exp_', exp_str, '_zs_cl_inf-test.csv', sep='')
  ent_compute_zs(null_rs_fname, zs_out_fname, zs_stats_fname)
}  

lapply(exp_strs, z_comps, data_path=data_path)
  
  
```

Lets look at those stats!

```{r}

z_stats <- lapply(c('lt', 'ts'), 
                function(x) read.csv(paste(res_path, 'exp_', x, '_zs_cl_inf-test.csv', sep=''), 
                                     header=T))

fmt = 'In line with the hypothesis that we could elicit routines with the task, routine z-scores were significantly lower than would be expected by chance in the first experiment, i.e. a z-score of -2 (t(%d) = %.2f, p< %.2f), and this replicated across experiments (t(%d) = %.2f, p< %.2f)'
z_results <- sprintf(fmt, z_stats[[1]]$parameter.df[[2]], z_stats[[1]]$statistic.t[2], z_stats[[1]]$p.value,
        z_stats[[2]]$parameter.df[[2]], z_stats[[2]]$statistic.t[2], z_stats[[2]]$p.value)


```

Now we want to visualise the z-scores, but we will also compare them to the performance of a perfectly performing but perfectly random agent, whose choices are similar constrained like the participant nulls (i.e. the same door cannot be picked twice in a row).

Note that the below code block does not run, owing to the comp time required to generate the nulls.

```{r, eval=FALSE}

agent_save_name <- paste(sim_data_path, 'random-agent_z-score-analysis.Rds', sep='')
get_random_agent_null(n_samples=1000, save_name = agent_save_name)
```

Now lets plot the histograms of humans vs random agent. Note this code produces a labelled pdf (humans v agent) for talks, and a pdf pared back for a manuscript.

```{r}

exp_strs <- c('lt', 'ts')
col_scheme <- c('#1b9e77','#d95f02', '#7570b3')
names(col_scheme) <- c(exp_strs, "agent")
sims_fname = paste(sim_data_path, 'random-agent_z-score-analysis.Rds', sep='/')
ms_z_plt_fname = paste(fig_path, 'z-hsts_ms', sep='')
tlk_z_plt_fname = paste(fig_path, 'z-hsts_tlk', sep='')
z_p_wdth <- 10 # plot width of ms plot, in cm
z_p_hgt <- z_p_wdth*(6/10)
produce_z_plts(exp_strs,
               col_scheme,
               z_p_wdth,
               z_p_hgt,
               data_path,
               sims_fname,
               ms_z_plt_fname,
               tlk_z_plt_fname)
```

![Fig 2: Showing z-scores for humans and random agent](figs/z-hsts_tlk.svg){fig-align="cent"}

Next, we can compare the mean z-score of the humans to that of the random agent - i.e. what is the probability that the mean z-score of the humans comes from the null distribution of the random agent?

```{r}

exp_strs <- c('lt', 'ts')
human_zs <- lapply(exp_strs, function(x) read.csv(paste(data_path,
                                                        'exp_', x,
                                                        '_zs_cl.csv', 
                                                        sep=''), header=T))

# this will load a n length vector called 'zs' where n = the number of times a null was generated for the random agent
load(paste(sim_data_path, 'random-agent_z-score-analysis.Rds', sep=''))

human_v_agent_zs <- sapply(human_zs, function(x) (mean(x[,'mu_z']) - mean(zs))/sd(zs))
names(human_v_agent_zs) <- exp_strs
human_v_agent_ps <- sapply(human_v_agent_zs, pnorm)
names(human_v_agent_ps) <- exp_strs

# make a dataframe of the results and save as a csv
hum_z_dat <- tibble(exp = exp_strs,
                    zs = human_v_agent_zs,
                    ps = human_v_agent_ps)
write.csv(hum_z_dat, file=paste(res_path, 'human_v_agent_zs.csv', sep=''),
          row.names=FALSE)

# print out the result
sprintf('the probability that the human data comes from the null distribution is p= %.2f for the lt exp, and p= %.2f for the ts exp', round(human_v_agent_ps['lt'],2), round(human_v_agent_ps['ts'],2))



```

### Routine scores are reliable across sessions

Now that we know we elicit routines above what you would expect by chance, we next ask, using the data from the dopamine study, how reliable are these routines within an individual?

```{r, eval=FALSE, results='hide'}

rel_data_fname <- paste(data_path, 'dat4_seq_model.Rda', sep='') # note: this datafile was generated by this project -https://github.com/garner-code/DA_VisRoutes and copied over to this project for the current analysis 
rel_data_save_name <- paste(data_path, 'rel_rs_wf.csv', sep='') # save the r's from da and placebo in widefrom
rel_analysis_save_name <- paste(res_path, 'da_reliability_analysis.csv', sep='')
reliability_analysis(rel_data_fname, rel_data_save_name,
                     rel_analysis_save_name)

# now we've made the data file, report the results of the reliability analysis
rel <- read.csv(rel_analysis_save_name)
fmt = 'the measure demonstrates some reliability, r = %.2f, 95 CI[%.2f, %.2f], t(%.0f) = %.2f, p = %.2f'
sprintf(fmt, rel$r, rel$l, rel$u, rel$df, rel$t, rel$p)
```

Now let's see the correlation in the data -

```{r, eval=FALSE}

# note the 'r' is for reliability
r_p_wdth <- z_p_wdth/2 # plot width of ms plot, in cm - its half the width of the z-score plot
r_p_hgt <- z_p_hgt
r_col = col_scheme[3]
fig_lab = 'C'
rel_data_save_name = rel_data_save_name
rel_plt_fname = paste(fig_path, 'reliability_data', sep='')
plot_rel(r_p_wdth, r_p_hgt, r_col,
         fig_lab, 
         rel_data_save_name,
         rel_plt_fname)
```

![Fig 3: Correlation of routine scores between sessions](figs/reliability_data_4tlks.svg){fig-align="cent"}

Now I can put Figs 1, 2 and 3 together to make one figure. Going to use the magick package, as dealing with pdfs.

```{r, eval=FALSE}

fnms = c(traj_fig_fname, ms_z_plt_fname, rel_plt_fname)

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

multi <- c(ims[[2]], ims[[3]]) %>%
         image_append() %>%
        c(ims[[1]]) %>%
         image_append(stack=TRUE)
traj_hght = (w/6*2)+.5 # get the height of the traj plot
pdf(paste(fig_path, 'FigA.pdf', sep=''), width=z_p_wdth+r_p_wdth,
    height=z_p_hgt+traj_hght)
plot(multi)
dev.off()


```

## Can we systematically manipulate how routine someone is?

Now we want to demonstrate that over our two experiments, our training manipulation of stable vs less stable contexts make people more or less routine. We then want to locate the source of disruption to the routine (i.e. what kind of responses are different between the two groups)

### How did the training manipulation affect routine scores over the two experiments?

To answer this, we need to visualise the routine scores by training group, across the two experiments.

```{r, results='hide'}
# first, I need to get the routine scores matched with the training group scores
lapply(c('lt', 'ts'), get_r_info_and_save, data_path=data_path)

plot_r_train(p_wdth = z_p_wdth, 
             p_hgt = z_p_hgt, 
             cols = col_scheme, 
             x_rng = c(0, 40),
             y_rng = c(0, 25),
             exp_strs = exp_strs,
             data_path = data_path,
             r_by_g_fname = paste(fig_path, 'rs_by-traintype_hists', sep=''))

```

![Fig 4: How training condition impacted routine scores](figs/rs_by-traintype_hists_4tlks.svg){fig-align="cent"}

As predicted, there is a systematic difference between training conditions. Now we can test the difference between them. Note that as the scores are clearly skewed, we will log transform the r scores prior to performing a t-test to compare the two groups in each experiment.

```{r, results='hide'}

get_rdat_and_label <- function(exp_str, data_path){
  rdat <- get_dat(exp_str, data_path)
  rdat$exp = exp_str
  rdat
}

rdat <- do.call(rbind, lapply(exp_strs, get_rdat_and_label, data_path = data_path))

do_grp_t <- function(rdat, exp_str){
  with(rdat %>% filter(exp == exp_str),
       t.test(log(r) ~ train_type))
}

get_cohens <- function(rdat, exp_str){
  tmp = rdat %>% filter(exp == exp_str) %>%
          mutate(logr = log(r))
  cohens_d(data = tmp,
           formula = logr~train_type,
           ci = TRUE,
           ci.type="perc",
           nboot = 1000)
}

grp_comps <- lapply(exp_strs, do_grp_t, rdat=rdat)
grp_cohens <- lapply(exp_strs, get_cohens, rdat=rdat)

prnt_t <- function(t_res, d_res){
  
  tibble(t = t_res$statistic,
         df = t_res$parameter,
         p = t_res$p.value,
         mu_diff = t_res$estimate[1] - t_res$estimate[2],
         mu_l = t_res$conf.int[1],
         mu_u = t_res$conf.int[2],
         d = d_res$effsize,
         d_l = d_res$conf.low,
         d_u = d_res$conf.high,
         dv = 'log r')
}

grp_r_comp <- do.call(rbind, 
                      lapply(1:length(grp_comps),
                             function(x) 
                               prnt_t(grp_comps[[x]],
                                      grp_cohens[[x]])))
grp_r_comp$exp <- exp_strs

# now save as a csv file 
write.csv(paste(res_path, 'grp_r_ts.csv', sep=''), row.names=FALSE)

```

The results are pleasingly consistent across both experiments, so will show the inferential outcomes in a table -

```{r}
#| label: routine by group tests
#| tbl-cap: "comparing routines by groups"

kable(grp_r_comp, digits=2)

```

### What kinds of responses account for the differences in routine observed between the groups, across the two experiments?

This is where we look at the task-jumps measure between the two groups. Here I create a group x trial type dataframe and draw and save the boxplots.

```{r, results='hide'}

# first, get the task jump data - note that I am not saving it as a csv,
# as it already exists with all the relevant info in the _avg csv
get_jump_data <- function(exp_str, data_path){
  
  tmp <- read.csv(paste(data_path, 'exp_', exp_str,
                        '_avg.csv', sep='')) %>%
    filter(ses == 2) %>%
    select(sub, train_type, context, switch, context_changes) %>%
    group_by(sub, train_type, switch) %>% 
    summarise(jumps=mean(context_changes)) %>%
    ungroup()
  tmp$exp = exp_str
  tmp
}

jumps <- do.call(rbind, lapply(exp_strs, get_jump_data,
                               data_path = data_path))

j_wdth = z_p_wdth
j_hgt = z_p_hgt
col_scheme = c('#7570b3', '#e7298a')

gen_jumps_plot(jumps,
               'jumps ~ switch*train_type', 
               exp_strs,
               col_scheme,
               j_wdth, j_hgt,
               paste(fig_path, 'task-jumps', sep=''),
               fig_labs = c('A', 'B'),
               ylabel = 'Jumps',
               xlabel = 'Group',
               ylims=c(0,5))

```

Now lets see the fig in all its glory -

![Fig 5: People in the frequent switch group broke up their routines to jump tasks](figs/task-jumps_4tlks.svg){fig-align="cent"}

Now I will put the routine by group and the task jumps figure into one plot for the manuscript.

```{r}

fnms = c(paste(fig_path, 'rs_by-traintype_hists', sep=''),
         paste(fig_path, 'task-jumps', sep=''))

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

multi <- c(ims[[1]]) %>%
         image_append() %>%
        c(ims[[2]]) %>%
         image_append(stack=TRUE)

pdf(paste(fig_path, 'FigB.pdf', sep=''), width=j_wdth,
    height=j_hgt)
plot(multi)
dev.off()

```

It looks like there is an interaction between group and switch (in fact we know there is, from Emily & Dinuk's analysis). Here I replicate their results: -

```{r, results='hide'}

# first set the factors to be factors
jumps$sub <- as.factor(jumps$sub)
jumps$train_type <- as.factor(jumps$train_type)
levels(jumps$train_type) <- c('stable','variable')
jumps$switch <- as.factor(jumps$switch)
levels(jumps$switch) <- c('stay','switch')

# now run the ANOVA model
# set emmeans option to multivariate
afex_options(emmeans_model = "multivariate")
# perform the statistical model
exp_strs <- unique(jumps$exp)
tj_aovs <- lapply(exp_strs, function(x) 
  aov_ez("sub", "jumps", jumps %>% filter(exp == x), 
         within = "switch",
         between = "train_type"))
names(tj_aovs) <- exp_strs

# convert the anovas to a dataframe, save the results and show as a table
convert_aov_to_df <- function(aovl, exp_str){
  ref = aovl$anova_table
  tibble(exp = rep(exp_str, length(ref$F)),
         effect = rownames(ref),
         numDF = ref$`num Df`,
         denDF = ref$`den Df`,
         Fstat = ref$F,
         ges = ref$ges,
         p = ref$`Pr(>F)`)
}
tj_aov_dat <- do.call(rbind, 
                      lapply(1:length(exp_strs),
                             function(x) convert_aov_to_df(tj_aovs[[x]],
                                                           exp_strs[x])))
write.csv(tj_aov_dat, paste(res_path, 'task-jumps_aov.csv'), row.names=FALSE)

```

```{r}

#| label: impact of condition on routines
#| tbl-cap: "impact of switch condition on routines by group"

kable(tj_aov_dat, digits=2)

```

Now I need to follow up the interactions, and grab the estimated marginal means so I can report the main effects.

Getting the desc stats on the main effect of group:

```{r}

# first I get the between group effect
me_grp <- lapply(tj_aovs, function(x) summary(emmeans(x, 'train_type')))
me_grp <- do.call(rbind, me_grp)
me_grp$exp <- rep(exp_strs, each=length(unique(me_grp$train_type)))
names(me_grp) <- c('group', 'emmean', 'se', 'df', 'l', 'u', 'exp')
write.csv(me_grp, paste(res_path, '/task-jumps_ph_me-grp.csv', sep=''))

```

```{r}

#| label: main effect of group - estimated marginal means
#| tbl-cap: "main effect of group - estimated marginal means"
kable(me_grp, digits=2)
```

And the simple effects of switch, at each level of group:

```{r}

# and now I test the following 
simp_fx <- lapply(tj_aovs, function(x) 
           list(summary(emmeans(x, 'switch', by='train_type'))))
sw_by_tt <- do.call(rbind, lapply(simp_fx, function(x) summary(x[[1]])))
sw_by_tt$exp <- rep(c('lt','ts'), each = 4)
names(sw_by_tt) <- c('group', 'switch', 'emmean', 
                     'se', 'df', 'l', 'u', 'exp')
write.csv(sw_by_tt, file=paste(res_path, 'task-jumps_ph_se-sw.csv', sep=''), row.names=FALSE)


```

```{r}

#| label: simple effects of switch at each level of group
#| tbl-cap: "simple effects of switch at each level of group"
kable(sw_by_tt)
```

We can conclude at this point that frequent switching makes people more variable in the order in which they perform their behaviours, and that this might be because they are jumping between the two tasks more frequently. This may be because they are mixing up the two tasks more. We also want to rule out general confusion. To do this, we look at responses that come from neither task.

```{r}

# get general errors data
get_error_data <- function(exp_str, data_path){
  
  tmp <- read.csv(paste(data_path, 'exp_', exp_str,
                        '_avg.csv', sep='')) %>%
    filter(ses == 2) %>%
    select(sub, train_type, context, switch, general_errors) %>%
    group_by(sub, train_type, switch) %>% 
    summarise(errors=mean(general_errors)) %>%
    ungroup()
  tmp$exp = exp_str
  tmp
}

errors <- do.call(rbind, lapply(exp_strs, get_error_data,
                                data_path = data_path))

gen_jumps_plot(errors,
               'errors ~ switch*train_type',
               exp_strs,
               col_scheme,
               j_wdth, j_hgt,
               paste(fig_path, 'general-errors', sep=''),
               fig_labs = c('C', 'D'),
               ylabel = 'Errors',
               ylims=c(0,1),
               xlabel = 'Group')


```

![Fig 5: People in the two groups showed the same levels of out of task errors](figs/general-errors_4tlks.svg){fig-align="cent"}

Next, I will put the task jumps and error figures together, for the first paper.

```{r}

fnms = c(paste(fig_path, 'task-jumps', sep=''), 
         paste(fig_path, 'general-errors', sep=''))

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

multi <- c(ims[[1]]) %>%
         image_append() %>%
        c(ims[[2]]) %>%
         image_append(stack=TRUE)
plt_hgt = j_hgt*2 # get the height of the traj plot
plt_wdth = j_wdth
pdf(paste(fig_path, 'jumps-and-errors.pdf', sep=''), width=plt_wdth,
    height=plt_hgt)
plot(multi)
dev.off()
```

Now I analyse errors in the same way as I did task jumps. Note that after correcting for multiple tests, only the main effect of switch is consistently significant across the two experiments.

```{r, results='hide'}

# first set the factors to be factors
errors$sub <- as.factor(errors$sub)
errors$train_type <- as.factor(errors$train_type)
levels(errors$train_type) <- c('stable','variable')
errors$switch <- as.factor(errors$switch)
levels(errors$switch) <- c('stay','switch')

# perform the statistical model
exp_strs <- unique(errors$exp)
er_aovs <- lapply(exp_strs, function(x) 
  aov_ez("sub", "errors", errors %>% filter(exp == x), 
         within = "switch",
         between = "train_type"))
names(er_aovs) <- exp_strs

er_aov_dat <- do.call(rbind, 
                      lapply(1:length(exp_strs),
                             function(x) convert_aov_to_df(er_aovs[[x]],
                                                           exp_strs[x])))
write.csv(er_aov_dat, paste(res_path, 'gen-errs_aov.csv'), row.names=FALSE)

```

```{r}

#| label: effect of group and switch on general errors
#| tbl-cap: "effect of group and switch on general errors"
kable(er_aov_dat, digits=2)
```

Because of that, I'll just get the emms on the me of switch.

```{r}

me_sw_er <- lapply(er_aovs, function(x) summary(emmeans(x, 'switch')))
me_sw_er <- do.call(rbind, me_sw_er)
me_sw_er$exp <- rep(exp_strs, each=length(unique(errors$train_type)))
names(me_sw_er) <- c('switch', 'emmean', 'se', 'df', 'l', 'u', 'exp')
write.csv(me_sw_er, paste(res_path, '/gen-errs_ph_sw.csv', sep=''))
```

```{r}

#| label: effect of switch on errors
#| tbl-cap: "effect that switch has on general errors"
kable(me_sw_er, digits=2)
```

OK, so now we know that the variable group showed less consistent behaviours in terms of order, but comparable accuracy to the stable group (the test is well powered and we can't discern a group difference).

So being in the variable group impacts your routines. The next question is why?

## Why did the stable vs variable manipulation impact routines?

The key thing we want to know is, did we mess up people's routines because they were probability matching to context, instead of tracking the probability of success, given the context?

Using Bayes theorem, we compute the probability that you are still in context A, given that you have observed that n number of doors from A, and m number of doors from B do not have a target behind them. We can use this probability to then compute the probability of success on your next go, given the number of doors left in each context.

Using Bayes theorem, we can cast the probability that you are still in $C_A$, given you have observed n and m false doors:

$$P(C = A | n, m) = \frac{ P(n,m|C=A)P(C=A)}{P(n,m)}$$

Once we have the probability of being in that context, given n and m, we can compute the probability of success for staying in that context, and the probability of success for moving to the other context, by taking the new probability of still being in context A, and multiplying it by the probability of success, given the number of doors left in A.

$$
P(S|C = A,n) = \frac{P(C=A|n,m)}{n_\mathrm{max} - n}
$$

where S is success, and $n_{\mathrm{max}}$ is the total number of n that can be chosen (i.e. the number of target doors in the set).

Note that we ignore repetitions when calculating these trial by trial probabilities.

We want to see how well each set of probabilities accounts for task jumps, over and above what can be accounted for by the number of switches a participant experiences (i.e. with increasing switches comes increasing chance of confusion), or the switch rate (how often a switch occurs on average, given experience).

Switches are the cumulative sum of experienced switches - i.e. how often you found a target in a context that is different to the last context where you found a target:

$$
\mathrm{Sw} = \sum_{i=1}^S S(i)
$$

And switch rate is simply the probability of a switch, given the number of targets you have found:

$$
\mathrm{Sw_{r}} = \frac{\mathrm{Sw}}{t}
$$

where t is the total number of targets found so far.

First lets get the data in shape. First we get the switch and switch rate variables -

```{r, eval=FALSE}

exp_strs <- exp_strs

get_sw_info <- function(exp_str){
  dat <- read.csv(paste(data_path, 'exp_', exp_str, '_evt.csv', sep='')) %>%
      filter(ses == 2) %>% 
      select(sub, train_type, t, context, door, door_cc, door_oc, door_nc, switch)

      #################################################
      # add Sw and Swr regressors for each subject
      subs <- unique(dat$sub)
      dat <- do.call(rbind, lapply(subs, get_Sw, dat=dat))
      dat$exp = exp_str
      dat
}

dat <- do.call(rbind, lapply(exp_strs, get_sw_info))
```

Now I will get $p(C = A | n, m)$ and $p(S|C,n)$. The function I call is a bit of a beast, but hopefully its sufficiently commented that its easy enough to follow what is happening. Basically for each trial, I calculate:

$p(C = A) = \frac{1}{t} \sum_{i=1}^{t} (C_i == C_{i-1})$

For each trial, I then count the number of n's and m's. Note that n and m are characterised according to the last place you found a target. e.g. if I just found a target from CA, but the true state of the world is now CB, any selection from CB would be counted as an m, as I have no evidence yet to tell me its an n. Note that each n and m is only counted once (duplicates are not counted).

For the cumulative counts of n and m on each trial, I assign the probability of that many nulls, given you are in the context to which those doors belong. Note the equation below is defined for n, but I also do the same for $p(m|C=B)$

$$
p(n | C=A) =  \frac{n_{max} - n}{n_{max}}
$$

This allows me to compute, based on trial by trial experience, $p(C = A | n, m)$ and $p(S|C=A, n,m)$. At the end, I then shift all the information down 1 row, as the current outcome should affect the next behaviour. For each regressor, I then take the odds by dividing by $p(C=B|n,m)$ and $p(S|C = B,n,m)$. I then use the odds variables in the subsequent logistic regressions.

```{r, eval=FALSE}

exp_strs <- exp_strs

get_pcanm_info <- function(exp_str, dat){
  #################################################
  # add Sw and Swr regressors for each subject
  subs <- unique(dat$sub)
  dat <- do.call(rbind, lapply(subs, get_p_context, dat=dat))
  dat
}

dat <- do.call(rbind, lapply(exp_strs, function(x) get_pcanm_info(x, dat %>% filter(exp == x))))
write.csv(dat, paste(data_path, 'evt-dat_4log-reg.csv', sep=''), row.names=FALSE)
rm(dat)
```

Now I have the model-based regressors, I use them in a logistic regression to predict $m$ selections for each individual. Note that I scale the predictors before running the model.

```{r, results='hide'}

betas_fname = paste(data_path, 'evt-dat_4log-reg.csv', sep='')
exp_strs <- c('lt', 'ts')

lapply(exp_strs, get_logist_mods_and_betas, fname=betas_fname, 
       data_path=data_path, res_path=res_path)
```

Note that I played with plotting the individual level fits, and while I was happy the models were doing a good enough capturing individual variance, I am yet to find a satisfying way to plot the individual model fits (owing to the multiple continuous predictors). So I will skip straight to the second level analysis, where we take the parameters from the logistic regression models and compare between groups.

```{r, eval=FALSE}
# some notes re: cleaning. For some participants, the algorithm did not converge,  or the estimated probabilties could not be distinguished between 0 and 1. Eyeballing the boxplots, these subjects largely fell outside the > 1.5 x the IQR (although sometimes the IQR excluded extra participants for whom there was no warning). Removing outliers > 1.5 x IQR seems the most objective way to treat the data, to me.

apply_clean_and_analyse_betas <- function(exp_str, data_path, res_path){
  
  betas <- read.csv(paste(data_path, 
                          'betas_', exp_str, '_first-level.csv',
                          sep=''))
  betas <- apply_outlier_filter_to_all_vars(betas)
  # save the new data file, as it has extra variables
  write.csv(betas, file=paste(data_path, 'betas_',
                              exp_str, '_first-level_cln.csv',
                              sep=''))
  apply_t_tests_to_all_vars(betas, exp_str, res_path) # this applies inferential t-tests and writes the results to csv files in res_path
}
lapply(c('lt', 'ts'), apply_clean_and_analyse_betas, 
                      data_path = data_path,
                      res_path = res_path)

```

I don't present the beta inferential tests here, as there are so many, but the take home is that they all betas are significantly above zero, the mu is clearly sig between the two groups, which we know, there is some glimpse of an impact of p(scs\|cntxt), but we maybe need more data, or a model that adapts to learning rate to determine for sure.

Next step is to blot the beta coefficients by group (and analysis)

```{r}

# now that we have analysed the betas ()
# plan is to do box plots
# top row = co-effs by parameter, by group # lt
# second row = co-effs by parameter, by group # ts

make_coefs_plts_4paper_andtlks(data_path,
                               paste(fig_path, 
                                     'betas_scnd-lvl',
                                     sep=''),
                               p_wdth=12,
                               p_hgt=12)

```

Now lets see the results:

::: {#fig-betas layout-ncol="1"}
![](figs/betas_scnd-lvl_4tlks.svg)

Showing the betas by group and parameter
:::

So it looks like all of the tested factors contributed to the increase of other context door selections, which differed by groups (who differed by routine). It looks like the variable group was generally more confused than the other group, with a small suggestion that they may have also been matching to the probability of the context.

As an exploratory plot, I plot routine scores against beta coefficients, to check the correlations between them appear to do what we think -

```{r}

exp_str = 'lt'
r_dat = read.csv(paste(data_path, 'exp_', exp_str, '_rscore.csv', sep=''))
betas = read.csv(paste(data_path, 'betas_', exp_str, '_first-level_cln.csv',
                                sep=''))
betas_r <- inner_join(betas %>% select(c(sub, train_type, ends_with('_flt'))),
                      r_dat, by='sub')
betas_r <- betas_r %>% na.omit() %>% mutate(r = log(r))

# first I'll check for multivariate outliers
mhl.mat <- as.matrix(betas_r[,c('mu_flt', 'scs_flt', 'cntx_flt','r')]) 
mhl.cov <- cov(mhl.mat) # here I get the covariance matrix
mhl.dist <- mahalanobis(mhl.mat, colMeans(mhl.mat), mhl.cov) # now calc the M dist
cut_off <- qchisq(.001, df=length(mhl.dist)-1)
#hist(mhl.dist)
sum(mhl.dist > cut_off) # we keep all the data

# prepare for the pairs plot
betas_r$train_type <- as.factor(betas_r$train_type)
levels(betas_r$train_type) <- c('stable', 'variable')
svg(paste(fig_path, 'pairs4backup.svg', sep=''), height=7, width=7)
g <- ggpairs(betas_r %>% select(mu_flt, scs_flt, cntx_flt, r),
        mapping=ggplot2::aes(colour = betas_r$train_type),
        upper = list(continuous = wrap("cor", method = "spearman")))
print(g)
dev.off()
```

What this plot tells us is that the more that the probability of context increases your log odds of hitting a door from the other context, the less that the probability of success given context influences the probability of hitting a door from the other context. This is good. It tells us that the parameters trade off against each other in a way we would expect. (Note that I don't include sw and swr, for visual clarity. Comfortingly, the correlations between these parameters and the others were not significant (nor with the routine scores.))

To understand the relationship between R scores and the remaining predictors, here is an optional lm to run. I am not concluding much from it as everything is calculated on the same data, but it does tell us that we can't trust the direction of the first order correlations with r, so I am not going to over interpret the direction here. But we do see that these predictors correlate with the routine score.

```{r, eval=FALSE}

summary(with(betas_r, lm(r ~ mu_flt + scs_flt + cntx_flt)))
```

## Does the formation of routines attenuate capacity to transfer learning, or promote task switching performance?

Now the cherry on top. We ask whether the extent to which you became routine (the lower your r score) predicts your difficulty being flexible, or corresponds to you being better at task switching.

### Learning transfer/flexibility

First steps are to wrangle the routine and group x transfer task data, and looking at learning onset for each group and task as a boxplot. Note that from visual inspection of qqplots, I decided to log transform the data, to improve normality. ThenI take the ratio of the scores from each task and correlate with the routine score, then an inferential test of the things.

```{r}

# wrangle data
r_dat <- read.csv(paste(data_path, 'exp_lt_rscore.csv', sep=''))
onset_dat <- read.csv(paste(data_path, 'exp_lt_maggi-k4.csv', sep='')) %>% 
  select(sid, ses, transfer, k4_onset) %>% 
  filter(ses == 3) %>%
  mutate(transfer=recode(transfer, `1` = 'comp', `2` = 'part')) %>%
  pivot_wider(names_from=transfer, values_from=k4_onset) %>% 
  mutate(k4 = part / (comp + part))
names(onset_dat)[names(onset_dat) == "sid"] = "sub"
# proportion makes it more normal

# some participants never learned, so we lose anyone who has a k4 of NaN,
# as that means their score was Inf
onset_dat <- onset_dat %>% na.omit() %>%
                filter(is.finite(comp))# this gets rid of most, but still one to get rid of

# now join to the r dat
onset_dat <- inner_join(onset_dat, r_dat, by='sub')

# now I log my variables, and I load the avg data so that I can get the training 
# group variale
onset_dat <- onset_dat %>% mutate(log_r = log(r))
grp_inf <- read.csv(paste(data_path, 'exp_lt_avg.csv', sep='')) %>% 
  filter(ses == 3) %>%
  select(sub, train_type) %>%
  unique()
onset_dat <- inner_join(onset_dat, grp_inf, by='sub') %>%
                mutate(train_type=recode(train_type, `1`= 'stable', 
                                         `2` = 'variable'))
rm(grp_inf)

# Now I have the data, I want to replace any outliers with NA
dvs <- c('comp', 'part', 'k4', 'log_r')
onset_dat <- cbind(onset_dat,
         do.call(cbind, lapply(dvs, remove_outliers, betas=onset_dat)))

write.csv(onset_dat, paste(data_path, 'exp_lt_onsetwr.csv', sep=''), row.names=FALSE)
```

Now that I have the wrangled data, I will make 3 subplots and then combine them. The first is the main effect of partial v transfer, the second is the percentage difference by group, and the third is the correlation between r and the k4 bias measure.

```{r}

# first, I make the comp/part data longer so I can plot it
tmp <- onset_dat %>% select(sub, comp, part, train_type) %>%
          pivot_longer(cols=c('comp', 'part'), 
                       values_to = 'k4_on',
                       names_to='transfer')

col_scheme = c('#ccba72ff', '#4c5454ff')

k4_by_grp_fname = paste(fig_path,'k4_by_grp', sep='')
trn_bp_wdth = z_p_wdth*.8
trn_bp_hgt = z_p_hgt
plt_tran_bp_4paper_andtlks(k4_by_grp_fname,
                trn_bp_wdth,
                trn_bp_hgt,
                tmp,
                'k4_on ~ transfer*train_type', 
                col_scheme,
                'Onset',
                c(0, 200),
                fig_lab = 'A')

```

![Knowledge onset of 4 doors in the transfer tasks, by group.](figs/k4_by_grp_4tlks.svg){fig-align="cent"}

Now I want to plot the bias score, by group.

```{r}

bias <- onset_dat %>% select(sub, train_type, log_r_flt, k4_flt) %>%
               na.omit()
col_scheme <- wesanderson::wes_palette('IsleofDogs1')[5:6]
this_form = 'k4_flt ~ train_type'
ylabel = 'Bias'
fig_lab = 'B'

bs_wdth = z_p_wdth*.65
bs_hgt = trn_bp_hgt

bias_by_grp_fname = paste(fig_path, 'bias-by-grp', sep='')
plt_bias_by_grp_4paper_andtlks(bias_by_grp_fname,
                               bs_wdth, bs_hgt,
                               bias,
                               this_form,
                               col_scheme,
                               ylabel,
                               fig_lab)
```

![Transfer bias score, by group.](figs/bias-by-grp_4tlks.svg){fig-align="cent"}

Now, combine the boxplot and the bias scores into one figure.

```{r}
fnms = c( paste(fig_path, 'k4_by_grp', sep=''), 
          paste(fig_path, 'bias-by-grp', sep=''))

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

multi <- c(ims[[1]]) %>%
         image_append() %>%
        c(ims[[2]]) %>%
         image_append()
plt_hgt = trn_bp_hgt # get the height of the traj plot
plt_wdth = trn_bp_wdth+bs_wdth
pdf(paste(fig_path, 'k4_bias.pdf', sep=''), width=plt_wdth,
    height=plt_hgt)
plot(multi)
dev.off()
```

And now, the last piece, the correlation between the bias score and the logged routine variability score.

```{r}

lt_bias_corr_fname = paste(fig_path, 'lt-bias_r_corr', sep='')
plt_r_bias_cor_4paper_andtlks(lt_bias_corr_fname,
                              bs_wdth, bs_hgt,
                              bias,
                              'C')

```

![Transfer bias correlates with routine score.](figs/lt-bias_r_corr_4tlks.svg){fig-align="cent"}

Now put them all together into one mega plot!

```{r}

fnms = c(k4_by_grp_fname, bias_by_grp_fname, lt_bias_corr_fname)

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

bottom <- image_append(c(ims[[2]],ims[[3]]))
multi <- image_append(c(ims[[1]], bottom), stack=TRUE)
pdf(paste(fig_path, 'LT_results.pdf', sep=''), 
    width=trn_bp_wdth,
    height=bs_hgt*2)
plot(multi)
dev.off()

```

Ahhh. Done. Now for stats, I only want the difference in complete vs partial, the difference in the bias between groups, and the correlation between the r and the bias.

```{r}

# learning transfer inferences
me_transfer = with(onset_dat, t.test(comp_flt, part_flt, 
                                     paired=TRUE, na.rm=TRUE))
transfer_for_cohens <- onset_dat %>% select(sub, comp_flt, part_flt) %>%
                         na.omit() %>%
                         pivot_longer(cols=c('comp_flt', 'part_flt'), names_to='cond',
                                             values_to='n') 
me_transfer_d = cohens_d(data=transfer_for_cohens,
                         formula=n~cond, paired=TRUE,
                         ci=TRUE, ci.type='perc', nboot=1000)
grp_bias = with(onset_dat, t.test(k4_flt~train_type, na.rm=TRUE))
grp_bias_d = cohens_d(data=onset_dat %>% na.omit(k4_flt),
                      formula=k4_flt~train_type, paired=FALSE,
                      ci=TRUE, ci.type='perc', nboot=1000)
r_bias_cor = with(onset_dat, cor.test(k4_flt, log_r_flt, na.rm=TRUE))


lt_ts <- tibble(fx = c('transfer', 'group-bias'),
                t = c(me_transfer$statistic, grp_bias$statistic),
                df = c(me_transfer$parameter, grp_bias$parameter),
                p = c(me_transfer$p.value, grp_bias$p.value),
                mu_a = c(me_transfer$estimate[1], grp_bias$estimate[1]),
                mu_b = c(me_transfer$estimate[2], grp_bias$estimate[2]),
                l_fx = c(me_transfer$conf.int[1], grp_bias$conf.int[1]),
                u_fx = c(me_transfer$conf.int[2], grp_bias$conf.int[2]),
                d = c(me_transfer_d$effsize, grp_bias_d$effsize),
                dl = c(me_transfer_d$conf.low, grp_bias_d$conf.low),
                du = c(me_transfer_d$conf.high, grp_bias_d$conf.high),
                n1 = c(me_transfer_d$n1, grp_bias_d$n1),
                n2 = c(me_transfer_d$n2, grp_bias_d$n2))
write.csv(lt_ts, paste(res_path, 'lt_inf_tests.csv', sep=''), row.names=FALSE)
kable(lt_ts, digits=2)
```

And now save and print the correlation -

```{r}

lt_r_cor_info <- tibble(r=r_bias_cor$estimate,
                        l=r_bias_cor$conf.int[1],
                        u=r_bias_cor$conf.int[2],
                        t=r_bias_cor$statistic,
                        df=r_bias_cor$parameter,
                        p=r_bias_cor$p.value)
write.csv(lt_r_cor_info, paste(res_path, 'lt-r_corr.csv', sep=''), row.names=FALSE)
kable(lt_r_cor_info, digits=2)
```

Note to self - may want to also show accuracy data to demonstrate the robustness of the effect.

### Task switching

And now we ask exactly the same kinds of questions about task switching performance.

```{r}

# wrangle data
r_dat <- read.csv(paste(data_path, 'exp_ts_rscore.csv', sep=''))
ts_dat <- read.csv(paste(data_path, 'exp_ts_avg.csv', sep='')) %>% 
  filter(ses==3) %>%
  select(sub, context, switch, train_type, setting_errors,
         rt_correct, accuracy, general_errors) %>% 
  group_by(sub, switch, train_type) %>%
  summarise(se=mean(setting_errors),
            rt=mean(rt_correct),
            acc=mean(accuracy),
            ge=mean(general_errors)) %>%
  ungroup() %>%
  mutate(train_type=recode(train_type, `1` = 'stable', `2` = 'variable'),
         switch=recode(switch, `0` = 'stay', `1` = 'switch'))

# now I temporarily make a wider format dataframe so I can make the bias scores
tmp <- ts_dat %>% pivot_wider(id_cols=c(sub,train_type),
                              names_from=switch,
                              values_from=c(se, rt, acc)) %>%
                  mutate(se_bias = se_switch/(se_stay+se_switch),
                         rt_bias = rt_switch/(rt_stay+rt_switch),
                         acc_bias = acc_switch/(acc_stay+acc_switch))
# now join key variables to other dataframe and remove the tmp one
ts_dat <- inner_join(ts_dat, tmp %>% select(sub, se_bias, rt_bias, acc_bias), by='sub')
rm(tmp)
# now join to the r dat and log the r variable
ts_dat <- inner_join(ts_dat, r_dat, by='sub') %>%
            mutate(log_r = log(r))

# Now I have the data, I want to replace any outliers with NA
# Note that setting errors wound up in a lot of dropped data, so I am not going to use the filtered versions of that variable
dvs <- c('se', 'rt', 'acc', 'se_bias', 'rt_bias', 'acc_bias', 'log_r')
ts_dat <- cbind(ts_dat,
                do.call(cbind, lapply(dvs, remove_outliers, betas=ts_dat)))
write.csv(ts_dat, paste(data_path, 'exp_lt_tswr.csv', sep=''), row.names=FALSE)
```

Now I make 3 very similar plots as I did for learning transfer. But I have to do this twice for the task switching data - once for setting errors and once for RT

```{r}

# setting errors plot
ts_dat_4_plt <- ts_dat %>% 
  select(sub, train_type, switch, se)

col_scheme = c('#7570b3', '#e7298a')

ts_bp_g_se_fname = paste(fig_path, 'se_by_grp', sep='')
ts_bp_wdth = trn_bp_wdth 
ts_bp_hgt = trn_bp_hgt 
plt_ts_bp_4paper_andtlks(ts_bp_g_se_fname,
                         ts_bp_wdth, ts_bp_hgt,
                         ts_dat_4_plt,
                         'se ~ switch + train_type',
                         col_scheme,
                         ylabel='Set-Errors',
                         ylims=c(0, 0.6), # sort axes
                         yticks=seq(0, 0.6, by = 0.2),
                         fig_lab='C',
                         legend_on=TRUE)
```

![Task-switching stage setting errors, by group.](figs/se_by_grp_4tlks.svg){fig-align="cent"}

And now the RTs:

```{r}

# setting errors plot
ts_dat_4_plt <- ts_dat %>% 
  select(sub, train_type, switch, rt_flt)

col_scheme = c('#7570b3', '#e7298a')

ts_bp_g_rt_fname = paste(fig_path, 'rt_by_grp', sep='')

plt_ts_bp_4paper_andtlks(ts_bp_g_rt_fname,
                         ts_bp_wdth, ts_bp_hgt,
                         ts_dat_4_plt,
                         'rt_flt ~ switch + train_type',
                         col_scheme,
                         ylabel='RT',
                         ylims=c(0.4, 1.8),
                         yticks=seq(0.4, 1.8, by = 0.4),
                         fig_lab='A',
                         legend_on=FALSE)

```

![Task-switching stage RT, by group.](figs/rt_by_grp_4tlks.svg){fig-align="cent"}

Now plot bias scores for both setting errors and RTs

```{r}

bias_se <- ts_dat %>% select(sub, train_type, se_bias) %>%
               na.omit() %>% unique()
col_scheme <- wesanderson::wes_palette('IsleofDogs1')[5:6]
this_form = 'se_bias ~ train_type'
ylabel = 'SE-Bias'
fig_lab = 'D'

bs_wdth = ts_bp_wdth*.7
bs_hgt = ts_bp_hgt

se_bias_by_grp_fname = paste(fig_path, 'exp_ts_se-bias-by-grp', sep='')
plt_bias_by_grp_4paper_andtlks(se_bias_by_grp_fname,
                               bs_wdth, bs_hgt,
                               bias_se,
                               this_form,
                               col_scheme,
                               ylabel,
                               fig_lab)
```

![Setting error bias by group.](figs/exp_ts_se-bias-by-grp_4tlks.svg){fig-align="cent"} And now plot the bias score for the RT measure.

```{r}

bias_rt <- ts_dat %>% select(sub, train_type, rt_bias_flt) %>%
               na.omit() %>% unique()
col_scheme <- wesanderson::wes_palette('IsleofDogs1')[5:6]
this_form = 'rt_bias_flt ~ train_type'
ylabel = 'RT-Bias'
fig_lab = 'B'

rt_bias_by_grp_fname = paste(fig_path, 'exp_ts_rt-bias-by-grp', sep='')
plt_bias_by_grp_4paper_andtlks(rt_bias_by_grp_fname,
                               bs_wdth, bs_hgt,
                               bias_rt,
                               this_form,
                               col_scheme,
                               ylabel,
                               fig_lab)
```

And combine into one big plot for showing the world.

```{r}
fnms = c( paste(fig_path, 'rt_by_grp', sep=''), 
          paste(fig_path, 'exp_ts_rt-bias-by-grp', sep=''), 
          paste(fig_path, 'se_by_grp', sep=''),
          paste(fig_path, 'exp_ts_se-bias-by-grp', sep=''))

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

multi <- ims[[1]] %>% c(ims[[2]]) %>%
          image_append() %>%
        c(ims[[3]] %>%c(ims[[4]]) %>% image_append()) %>%
          image_append(stack=TRUE)
plt_hgt = bs_hgt*2 # get the height of the traj plot
plt_wdth = ts_bp_wdth+bs_wdth
pdf(paste(fig_path, 'rt-and-se.pdf', sep=''), width=plt_wdth,
    height=plt_hgt)
plot(multi)
dev.off()
```

And now I plot the correlation between the setting error bias and the routine score -

```{r}

bias_se <- inner_join(bias_se, 
                      ts_dat %>% select(sub, log_r_flt), by='sub') %>%
            unique()

se_bias_corr_fname = paste(fig_path, 'ts-se-bias_r_corr', sep='')
plt_r_se_bias_cor_4paper_andtlks(se_bias_corr_fname,
                                 bs_wdth, bs_hgt,
                                 bias_se,
                                'C')
```

![Setting error bias correlates with routine score.](figs/ts-se-bias_r_corr_4tlks.svg){fig-align="cent"} And now I put these figures together for the paper -

```{r}

fnms = c(ts_bp_g_se_fname, se_bias_by_grp_fname, se_bias_corr_fname)

ims = lapply(fnms, function(x) image_read_pdf(paste(x, '.pdf', sep='')))

bottom <- image_append(c(ims[[2]],ims[[3]]))
multi <- image_append(c(ims[[1]], bottom), stack=TRUE)
pdf(paste(fig_path, 'TS_results.pdf', sep=''), 
    width=trn_bp_wdth,
    height=bs_hgt*2)
plot(multi)
dev.off()

```

All the figures are done!

```{r}

# first get the switch setting errors to compare between
sw4t_dat <- ts_dat %>% select(sub, train_type, switch, se) %>%
  pivot_wider(names_from=switch, values_from = se) %>%
  na.omit()

switch_t = with(sw4t_dat, 
                t.test(x=stay, y=switch, paired=TRUE, na.rm=TRUE))

switch_for_cohens <- sw4t_dat %>% 
  pivot_longer(cols=c('stay', 'switch'), 
               names_to='cond',
               values_to='n') 
switch_d = cohens_d(data=switch_for_cohens,
                         formula=n~cond, paired=TRUE,
                         ci=TRUE, ci.type='perc', nboot=1000)

# now compare the groups for setting error bias
grp_bias = with(bias_se, t.test(se_bias~train_type, na.rm=TRUE))
grp_bias_d = cohens_d(data=bias_se,
                      formula=se_bias~train_type, paired=FALSE,
                      ci=TRUE, ci.type='perc', nboot=1000)
# now compute the correlation
r_se_bias_cor = with(bias_se, 
                  cor.test(se_bias, log_r_flt, 
                           method='spearman'))

ts_ts <- tibble(fx = c('sw', 'group-bias'),
                t = c(switch_t$statistic, grp_bias$statistic),
                df = c(switch_t$parameter, grp_bias$parameter),
                p = c(switch_t$p.value, grp_bias$p.value),
                mu_a = c(switch_t$estimate[1], grp_bias$estimate[1]),
                mu_b = c(switch_t$estimate[2], grp_bias$estimate[2]),
                l_fx = c(switch_t$conf.int[1], grp_bias$conf.int[1]),
                u_fx = c(switch_t$conf.int[2], grp_bias$conf.int[2]),
                d = c(switch_d$effsize, grp_bias_d$effsize),
                dl = c(switch_d$conf.low, grp_bias_d$conf.low),
                du = c(switch_d$conf.high, grp_bias_d$conf.high),
                n1 = c(switch_d$n1, grp_bias_d$n1),
                n2 = c(switch_d$n2, grp_bias_d$n2))
write.csv(ts_ts, paste(res_path, 'ts-se_inf_tests.csv', sep=''), row.names=FALSE)
kable(ts_ts, digits=2)
```

And now I just print out the correlation results.

```{r}


ts_se_r_cor_info <- tibble(r=r_se_bias_cor$estimate,
                           S=r_se_bias_cor$statistic,
                           df=nrow(bias_se)-2,
                           p=r_se_bias_cor$p.value)
write.csv(ts_se_r_cor_info, 
          paste(res_path, 'ts_se-r_corr.csv', sep=''),
          row.names=FALSE)
kable(ts_se_r_cor_info, digits=2)
```
